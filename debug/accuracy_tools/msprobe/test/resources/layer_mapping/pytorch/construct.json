{
    "Tensor.__add__.0.forward": "",
    "Tensor.__add__.1.forward": "",
    "Tensor.__or__.0.forward": "Module.module.module.language_model.embedding.word_embeddings.VocabParallelEmbedding.forward.0",
    "Distributed.all_reduce.0.forward": "Module.module.module.language_model.embedding.word_embeddings.VocabParallelEmbedding.forward.0",
    "Module.module.module.language_model.embedding.word_embeddings.VocabParallelEmbedding.forward.0": "Module.module.module.language_model.embedding.Embedding.forward.0",
    "NPU.npu_rms_norm.0.forward": "Module.module.module.language_model.encoder.layers.0.input_norm.RMSNorm.forward.0",
    "Module.module.module.language_model.encoder.layers.0.ParallelTransformerLayer.forward.0": "Module.module.module.language_model.encoder.ParallelTransformer.forward.0",
    "Module.module.module.language_model.encoder.layers.0.input_norm.RMSNorm.forward.0": "Module.module.module.language_model.encoder.layers.0.ParallelTransformerLayer.forward.0",
    "Module.module.module.language_model.encoder.layers.0.self_attention.ParallelAttention.forward.0": "Module.module.module.language_model.encoder.layers.0.ParallelTransformerLayer.forward.0",
    "Torch.cos.0.forward": "Module.module.module.language_model.encoder.layers.0.self_attention.ParallelAttention.forward.0",
    "NPU.npu_fusion_attention.0.forward": "Module.module.module.language_model.encoder.layers.0.self_attention.core_attention_flash.FlashSelfAttention.forward.0",
    "NPU.npu_fusion_attention.0.backward": "Module.module.module.language_model.encoder.layers.0.self_attention.core_attention_flash.FlashSelfAttention.backward.0"
}